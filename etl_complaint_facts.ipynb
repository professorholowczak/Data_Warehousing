{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/professorholowczak/Data_Warehousing/blob/main/etl_complaint_facts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "\n",
        "# Google Colab load modules for BigQuery\n",
        "%load_ext google.cloud.bigquery\n",
        "%load_ext google.colab.data_table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFPIuJ0kP9NI",
        "outputId": "a4373b21-3aca-4a4d-9ac0-9bd379b3838e"
      },
      "id": "CFPIuJ0kP9NI",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n",
            "The google.cloud.bigquery extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.cloud.bigquery\n",
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "cfa00868",
      "metadata": {
        "id": "cfa00868"
      },
      "outputs": [],
      "source": [
        "# ETL Complaint Facts\n",
        "# If using the native Google BigQuery API module:\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "# import credentials\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow\n",
        "import logging\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "46960187",
      "metadata": {
        "id": "46960187"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame\n",
        "# Set the name of the dimension\n",
        "fact_name = 'complaints'\n",
        "\n",
        "# Set the GCP Project, dataset and table name\n",
        "gcp_project = 'put-your-GCP-project-name-here'\n",
        "bq_dataset = 'nyc_311_complaints_dw'\n",
        "table_name = fact_name + '_fact'\n",
        "# Construct the full BigQuery path to the table\n",
        "fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
        "\n",
        "# Set the path to the source data files\n",
        "# For Linux use something like    /home/username/python_etl \n",
        "# For Mac use something like     /users/username/python_etl\n",
        "# file_source_path = 'c:\\\\Python_ETL'\n",
        "file_source_path = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "060f8796",
      "metadata": {
        "id": "060f8796"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "current_date = datetime.today().strftime('%Y%m%d')\n",
        "log_filename = \"_\".join([\"etl_complaint_fact_\",current_date])+\".log\"\n",
        "logging.basicConfig(filename=log_filename, encoding='utf-8', format='%(asctime)s %(message)s', level=logging.DEBUG)\n",
        "logging.info(\"=========================================================================\")\n",
        "logging.info(\"Starting ETL Run for complaint fact on date \"+current_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "bc1e72b6",
      "metadata": {
        "id": "bc1e72b6"
      },
      "outputs": [],
      "source": [
        "def load_csv_data_file(logging, file_source_path, file_name, df):\n",
        "    \"\"\"\n",
        "    load_csv_data_file\n",
        "    Accepts a file source path and a file name\n",
        "    Loads the file into a data frame\n",
        "    Exits the program on error\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    file_source = os.path.join(file_source_path, file_name)\n",
        "    logging.info(\"Reading source data file: %s\",file_source)\n",
        "    # Read in the source data file for the customers data\n",
        "    try:\n",
        "        df = pd.read_csv(file_source)\n",
        "        # Set all of the column names to lower case letters\n",
        "        df = df.rename(columns=str.lower)\n",
        "        logging.info(\"Read %d records from source data file: %s\",df.shape[0],file_source)\n",
        "        return df\n",
        "    except:\n",
        "        logging.error(\"Failed to read file: %s\", file_source)\n",
        "        # os._exit(-1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "436af56f",
      "metadata": {
        "id": "436af56f"
      },
      "outputs": [],
      "source": [
        "def transform_data(logging, df):\n",
        "    \"\"\"\n",
        "    transform_data\n",
        "    Accepts a data frame\n",
        "    Performs any specific cleaning and transformation steps on the dataframe\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"    \n",
        "    # Convert the date_of_birth to a datetime64 data type. 2012-08-21 04:12:16.827\n",
        "    logging.info(\"Managing data types.\")\n",
        "    df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], format='%m/%d/%Y')\n",
        "    # Convert the postal code into a string\n",
        "    df['postal_code'] =  df['postal_code'].astype(str)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "0d1bee48",
      "metadata": {
        "id": "0d1bee48"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_client(logging):\n",
        "    \"\"\"\n",
        "    create_bigquery_client\n",
        "    Creates a BigQuery client using the path to the service account key file\n",
        "    for credentials.\n",
        "    Returns the BigQuery client object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # bqclient = bigquery.Client.from_service_account_json(credentials.path_to_service_account_key_file)\n",
        "        bqclient = bigquery.Client(gcp_project)        \n",
        "        logging.info(\"Created BigQuery Client: %s\",bqclient)\n",
        "        return bqclient\n",
        "    except Exception as err:\n",
        "        logging.error(\"Failed to create BigQuery Client.\", err)\n",
        "        # os._exit(-1)\n",
        "    return bqclient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "3578f519",
      "metadata": {
        "id": "3578f519"
      },
      "outputs": [],
      "source": [
        "def upload_bigquery_table(logging, bqclient, table_path, write_disposition, df):\n",
        "    \"\"\"\n",
        "    upload_bigquery_table\n",
        "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
        "    Loads the data into the BigQuery table from the dataframe.\n",
        "    for credentials.\n",
        "    The write disposition is either\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.   \n",
        "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
        "        # Submit the job\n",
        "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)  \n",
        "        # Show the job results\n",
        "        job.result()\n",
        "    except Exception as err:\n",
        "        logging.error(\"Failed to load BigQuery Table.\", err)\n",
        "        # os._exit(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "3d37a312",
      "metadata": {
        "id": "3d37a312"
      },
      "outputs": [],
      "source": [
        "def bigquery_table_exists(table_path, bqclient):\n",
        "    \"\"\"\n",
        "    bigquery_table_exists\n",
        "    Accepts a path to a BigQuery table\n",
        "    Checks if the BigQuery table exists.\n",
        "    Returns True or False\n",
        "    \"\"\"    \n",
        "    try:\n",
        "        bqclient.get_table(table_path)  # Make an API request.\n",
        "        return True\n",
        "    except NotFound:\n",
        "        # print(\"Table {} is not found.\".format(table_id))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "79339f5c",
      "metadata": {
        "id": "79339f5c"
      },
      "outputs": [],
      "source": [
        "def query_bigquery_table(logging, table_path, bqclient, surrogate_key):\n",
        "    \"\"\"\n",
        "    query_bigquery_table\n",
        "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
        "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
        "    Returns the dataframe\n",
        "    \"\"\"    \n",
        "    bq_df = pd.DataFrame\n",
        "    # sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
        "    sql_query = 'SELECT * FROM `' + table_path + '`'\n",
        "    logging.info(\"Running query: %s\", sql_query)\n",
        "    bq_df = bqclient.query(sql_query).to_dataframe()\n",
        "    return bq_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "484a7699",
      "metadata": {
        "id": "484a7699"
      },
      "outputs": [],
      "source": [
        "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
        "    \"\"\"\n",
        "    add_surrogate_key  \n",
        "    Accepts a data frame and inserts an integer identifier as the first column\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Reset the index\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # Add the new surrogate key starting from offset\n",
        "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "de2d5c71",
      "metadata": {
        "id": "de2d5c71"
      },
      "outputs": [],
      "source": [
        "def add_update_date(df, current_date):\n",
        "    \"\"\"\n",
        "    add_update_date\n",
        "    Accepts a data frame and inserts the current date as a new field\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    df['update_date'] = pd.to_datetime(current_date)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "b2125588",
      "metadata": {
        "id": "b2125588"
      },
      "outputs": [],
      "source": [
        "def add_update_timestamp(df):\n",
        "    \"\"\"\n",
        "    add_update_timestamp\n",
        "    Accepts a data frame and inserts the current datetime as a new field\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    df['update_timestamp'] = pd.to_datetime('now', utc=True).replace(microsecond=0)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "0bcfdfc7",
      "metadata": {
        "id": "0bcfdfc7"
      },
      "outputs": [],
      "source": [
        "def build_new_table(logging, bqclient, table_path, df):\n",
        "    \"\"\"\n",
        "    build_new_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
        "    Add the surrogate key and a record timestamp to the data frame\n",
        "    Inserts the contents of the dataframe to the dimensional table.\n",
        "    \"\"\"\n",
        "    logging.info(\"Target table %s does not exit\", table_path)\n",
        "    upload_bigquery_table(logging, bqclient, table_path, \"WRITE_TRUNCATE\", df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "ac16f1ea",
      "metadata": {
        "id": "ac16f1ea"
      },
      "outputs": [],
      "source": [
        "def insert_existing_table(logging, bqclient, table_path, df):\n",
        "    \"\"\"\n",
        "    insert_existing_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
        "    Compares the new data to the existing data in the table.\n",
        "    Inserts the new/modified records to the existing table\n",
        "    \"\"\"\n",
        "    logging.info(\"Target table %s exits. Appending records.\", table_path)\n",
        "    upload_bigquery_table(logging, bqclient, table_path, \"WRITE_APPEND\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "e092289d",
      "metadata": {
        "id": "e092289d"
      },
      "outputs": [],
      "source": [
        "def dimension_lookup(logging, dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df):\n",
        "    \"\"\"\n",
        "    dimension_lookup\n",
        "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    logging.info(\"Lookup dimension %s.\", dimension_name)\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "    # Fetch the existing table\n",
        "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
        "    # print(bq_df)\n",
        "    # Melt the dimension dataframe into an index with the lookup columns\n",
        "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
        "    # print(m)\n",
        "    # Rename the \"value\" column to the surrogate key column name\n",
        "    m=m.rename(columns={\"value\":surrogate_key})\n",
        "    # Merge with the fact table record\n",
        "    df = df.merge(m, on=lookup_columns, how='left')\n",
        "    # Drop the \"variable\" column and the lookup columns\n",
        "    df = df.drop(columns=lookup_columns)\n",
        "    df = df.drop(columns=\"variable\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "56993b38",
      "metadata": {
        "id": "56993b38"
      },
      "outputs": [],
      "source": [
        "def date_dimension_lookup(logging, dimension_name='date', lookup_column='created_date', df=df):\n",
        "    \"\"\"\n",
        "    date_dimension_lookup\n",
        "    Lookup the lookup_columns in a date dimension and return the associated surrogate keys\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    logging.info(\"Lookup date dimension on column %s.\", lookup_column)\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "    # Fetch the existing table\n",
        "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
        "    bq_df[\"full_date\"] = pd.to_datetime(bq_df.full_date, format=\"%Y-%m-%d %H:%M:%S\")\n",
        "    # Return just the date portion\n",
        "    bq_df[\"full_date\"] = bq_df.full_date.dt.date\n",
        "\n",
        "    # Dates in the 311 data look like this: 2017-08-11T11:57:00.000\n",
        "    # Extract the date from 'created_date' column\n",
        "    df[lookup_column] = pd.to_datetime(df[lookup_column], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    # Return just the date portion\n",
        "    df[lookup_column] = df[lookup_column].dt.date\n",
        "    \n",
        "    # Melt the dimension dataframe into an index with the lookup columns\n",
        "    m = bq_df.melt(id_vars='full_date', value_vars=surrogate_key)\n",
        "    # Rename the \"value\" column to the surrogate key column name\n",
        "    m=m.rename(columns={\"value\":lookup_column+\"_dim_id\"})\n",
        "    \n",
        "    # Merge with the fact table record on the created_date\n",
        "    df = df.merge(m, left_on=lookup_column, right_on='full_date', how='left')\n",
        "\n",
        "    # Drop the \"variable\" column and the lookup columns\n",
        "    df = df.drop(columns=lookup_column)\n",
        "    df = df.drop(columns=\"variable\")\n",
        "    df = df.drop(columns=\"full_date\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_dimension_lookup(logging, dimension_name='time', lookup_column='created_date', df=df):\n",
        "    \"\"\"\n",
        "    time_dimension_lookup\n",
        "    Lookup the lookup_columns in the time dimension and return the associated surrogate key\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    logging.info(\"Lookup time dimension on column %s.\", lookup_column)\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "    \n",
        "    # Dates in the 311 data look like this: 2017-08-11T11:57:00.000\n",
        "    # We can strip off the time portion after the letter \"T\" to ge the hours and minutes\n",
        "    # time_dim_id = (hours*60)+minutes+1\n",
        "    # Example:  Time is 22:07  so  (22*60)+7+1 = 1328\n",
        "    # Extract the date from 'created_date' column and save it in a temporary column\n",
        "    df[lookup_column+\"_newdate\"] = pd.to_datetime(df[lookup_column], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    # Strip off the hours and minutes portions\n",
        "    df[lookup_column+\"_hours\"] = df[lookup_column+\"_newdate\"].dt.strftime(\"%H\").astype(int)\n",
        "    # df[lookup_column+\"_hours\"] = df[lookup_column+\"_hours\"].astype(int)\n",
        "    df[lookup_column+\"_minutes\"] = df[lookup_column+\"_newdate\"].dt.strftime(\"%M\").astype(int)\n",
        "    # df[lookup_column+\"_minutes\"] = df[lookup_column+\"_minutes\"].astype(int)\n",
        "    # Now assign the time_dim_id\n",
        "    df[surrogate_key] = (df[lookup_column+\"_hours\"]*60)+df[lookup_column+\"_minutes\"]+1\n",
        "    # Drop the lookup time columns\n",
        "    df = df.drop(columns=lookup_column+\"_newdate\")\n",
        "    df = df.drop(columns=lookup_column+\"_hours\")\n",
        "    df = df.drop(columns=lookup_column+\"_minutes\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "IgT7wHFEzdcV"
      },
      "id": "IgT7wHFEzdcV",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "55edebde",
      "metadata": {
        "scrolled": true,
        "id": "55edebde"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    df = pd.DataFrame\n",
        "    # Create the BigQuery Client\n",
        "    bqclient = create_bigquery_client(logging)\n",
        "    # Load in the data file\n",
        "    df = load_csv_data_file(logging, file_source_path, \"311_bicycle_complaints_2017.csv\", df)\n",
        "    # If city is empty, fill it in with NEW YORK\n",
        "    df.city = df.city.fillna('NEW YORK')\n",
        "    # Consider removing columns that we will never use  df.drop([....])\n",
        "\n",
        "    # Lookup the agency dimension record  agency_dim_id\n",
        "    df = dimension_lookup(logging, dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df)\n",
        "    \n",
        "    # Lookup the location dimension record  location_dim_id\n",
        "    df = dimension_lookup(logging, dimension_name='location', lookup_columns=['borough', 'city', 'incident_address', 'incident_zip', 'latitude', 'longitude'], df=df)\n",
        "\n",
        "    # Lookup the channel  dimension record  channel_dim_id\n",
        "    df = dimension_lookup(logging, dimension_name='channel', lookup_columns=['open_data_channel_type', 'status'], df=df)\n",
        "\n",
        "    # Lookup the complaint_type  dimension record  complaint_type_dim_id\n",
        "    df = dimension_lookup(logging, dimension_name='complaint_type', lookup_columns=['complaint_type', 'descriptor'], df=df)\n",
        "\n",
        "    # Lookup the time dimension record using the time part of the created_date\n",
        "    # Note - do this before looking up the date dimension\n",
        "    df = time_dimension_lookup(logging, dimension_name='time', lookup_column='created_date', df=df)\n",
        "\n",
        "    # Lookup the created_date dimension record  \n",
        "    df = date_dimension_lookup(logging, dimension_name='date', lookup_column='created_date', df=df)\n",
        "\n",
        "    # Lookup the closed_date dimension record\n",
        "    df = date_dimension_lookup(logging, dimension_name='date', lookup_column='closed_date', df=df)\n",
        "\n",
        "    # A list of all of the surrogate keys\n",
        "    # For transaction grain, also include the 'unique_key' column\n",
        "    surrogate_keys=['agency_dim_id','complaint_type_dim_id','channel_dim_id','location_dim_id','created_date_dim_id','closed_date_dim_id']\n",
        "\n",
        "    # Remove all of the other non-surrogate key columns\n",
        "    df = df[surrogate_keys]\n",
        "\n",
        "    # For daily snapshot grain we:\n",
        "    # 1) Add a 'complaint_count' fact\n",
        "    # 2) Use Group By to count up the number of complaints, per location, per agency, etc. per day\n",
        "    # For transaction grain add in the unique_key but skip the above two steps.\n",
        "    \n",
        "    # Add a complaint count (for daily snapshot grain)\n",
        "    df['complaint_count'] = 1\n",
        "    # Count up the number of complaints per agency, per location, etc. per day\n",
        "    df = df.groupby(surrogate_keys)['complaint_count'].agg('count').reset_index()\n",
        "    \n",
        "    # See if the target table exists\n",
        "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
        "    # If the target table does not exist, load all of the data into a new table\n",
        "    if not target_table_exists:\n",
        "        build_new_table(logging, bqclient, fact_table_path, df)\n",
        "    # If the target table exists, then perform an incremental load    \n",
        "    if target_table_exists:\n",
        "        insert_existing_table(logging, bqclient, fact_table_path, df)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the log file to see any errors\n",
        "!tail -35 etl_complaint_fact__20230504.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x7E4vYo7WCm",
        "outputId": "b1a8c6ad-a042-441c-b9fd-e92e86db0d1d"
      },
      "id": "3x7E4vYo7WCm",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-04 13:21:47,242 Running query: SELECT * FROM `handy-bonbon-142723.nyc_311_complaints_dw.location_dimension`\n",
            "2023-05-04 13:21:47,637 https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/handy-bonbon-142723/jobs?prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:47,907 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/queries/fb4e1cb6-adee-4e65-9e17-1ae72fbe5c7c?maxResults=0&location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:48,432 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anonb2229a813f18f2155f52725b9ecd14e8a297870c0058e0af3cfb8a0cc7e0690a' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDFdobGhMWEtsaHhvYxoCaXcaAmpk'.\n",
            "2023-05-04 13:21:48,954 Lookup dimension channel.\n",
            "2023-05-04 13:21:48,955 Running query: SELECT * FROM `handy-bonbon-142723.nyc_311_complaints_dw.channel_dimension`\n",
            "2023-05-04 13:21:49,448 https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/handy-bonbon-142723/jobs?prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:49,697 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/queries/fbe56576-d573-4053-a26f-df5e92ead482?maxResults=0&location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:50,193 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anon33d1d664f658d993cf705db8046a1d86d09b75614d1e045da5e017402690ba76' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDDF0eW1Cc3dPa1czRxoCaXcaAmpk'.\n",
            "2023-05-04 13:21:50,853 Lookup dimension complaint_type.\n",
            "2023-05-04 13:21:50,853 Running query: SELECT * FROM `handy-bonbon-142723.nyc_311_complaints_dw.complaint_type_dimension`\n",
            "2023-05-04 13:21:52,025 https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/handy-bonbon-142723/jobs?prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:52,270 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/queries/6d389ba4-7e82-4d48-a5e3-89719af7fa3a?maxResults=0&location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:52,722 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anon221d43687025304c431778ea6fbc051df14ac0f46d46720e6eff1161c2663ef7' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDGtydnZLWnZTcjUtUBoCaXcaAmpk'.\n",
            "2023-05-04 13:21:53,362 Lookup time dimension on column created_date.\n",
            "2023-05-04 13:21:53,385 Lookup date dimension on column created_date.\n",
            "2023-05-04 13:21:53,385 Running query: SELECT * FROM `handy-bonbon-142723.nyc_311_complaints_dw.date_dimension`\n",
            "2023-05-04 13:21:53,802 https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/handy-bonbon-142723/jobs?prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:54,042 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/queries/5ebfa1a5-806c-4d60-9d8e-fc92cdb71f1d?maxResults=0&location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:54,560 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anon5b3161bfc0e7fcdd8fc9a41de7b2c377de91c66cc8c34f8b8513e497cb42753b' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDHhUNFVuTjk1bmo3LRoCaXcaAmpk'.\n",
            "2023-05-04 13:21:55,239 Lookup date dimension on column closed_date.\n",
            "2023-05-04 13:21:55,240 Running query: SELECT * FROM `handy-bonbon-142723.nyc_311_complaints_dw.date_dimension`\n",
            "2023-05-04 13:21:55,727 https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/handy-bonbon-142723/jobs?prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:55,974 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/queries/7f274de0-b583-4961-bf5c-6da6ea96b475?maxResults=0&location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:56,391 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anon5b3161bfc0e7fcdd8fc9a41de7b2c377de91c66cc8c34f8b8513e497cb42753b' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDEl6TzhqbElFNTNmahoCaXcaAmpk'.\n",
            "2023-05-04 13:21:56,985 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/datasets/nyc_311_complaints_dw/tables/complaints_fact?prettyPrint=false HTTP/1.1\" 404 None\n",
            "2023-05-04 13:21:56,987 Target table handy-bonbon-142723.nyc_311_complaints_dw.complaints_fact does not exit\n",
            "2023-05-04 13:21:58,129 https://bigquery.googleapis.com:443 \"POST /upload/bigquery/v2/projects/handy-bonbon-142723/jobs?uploadType=multipart HTTP/1.1\" 200 1992\n",
            "2023-05-04 13:21:58,215 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/a75e7a2e-8a56-405f-afad-2b06f4c6d6d5?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:58,216 Retrying due to , sleeping 0.5s ...\n",
            "2023-05-04 13:21:58,820 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/a75e7a2e-8a56-405f-afad-2b06f4c6d6d5?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:58,822 Retrying due to , sleeping 0.8s ...\n",
            "2023-05-04 13:21:59,731 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/a75e7a2e-8a56-405f-afad-2b06f4c6d6d5?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-05-04 13:21:59,732 Retrying due to , sleeping 1.0s ...\n",
            "2023-05-04 13:22:00,838 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/a75e7a2e-8a56-405f-afad-2b06f4c6d6d5?location=US&prettyPrint=false HTTP/1.1\" 200 None\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}